# A new Challenge
### Previously...
We successfully calculated the mean stack of a set of images and the results looked like a lot of noise. 
Each image had a non-detection, but as we stacked them, the signal to noise ratio improved until we could eventually detect the underlying population of pulsars.

### Now...
We want to **improve the robustness** of the method to outliers by implementing a **median** stack: we want to sort the values of each pixel in our image and create a final image using the middle pixel.
This, in practice, however, consumes a huge load of resources and time. In other words, the solution didn't scale.

### Why wouldn't the median work?

Back of the envelope calculations work best if you give them a go first.

```ad-tip
color: 212, 155, 84
icon: lightbulb
title: Back of the envelope
 The idea of the calculations is to make reasonable estimates for each step in the process and give you an order of magnitude answer to your computing and memory requirements. 

The **exact numbers don't matter**, we can use rough values to make the calculation easier.
```
Back of the envelopes work best if you give them a go first. 

### Back of the envelope calculation!
For starters, we have $600 000$ images of non-detections. Each image has $200 \cdot 200 = 40 000$ pixels.
Now, let's say that each data point is stored as an $8$ byte double floating point value.

If we calculate the median naively, we need to store all of this information at once, which results in: 

$$
600 000 \cdot 40 000 \cdot 8 = 192 \text{GB}.
$$

Yikes.
That's equal to the total memory available on a typical machine!!

### What now?

What are some of the ways we could fix this problem or avoid it entirely?
That's the question we will look at. In the next module, we'll also look at how to time your programs and evaluate their efficiency and scalability.

